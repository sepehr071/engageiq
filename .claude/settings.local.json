{
  "permissions": {
    "allow": [
      "mcp__web-search-prime__webSearchPrime",
      "mcp__web-reader__webReader",
      "mcp__livekit-docs__code_search",
      "mcp__livekit-docs__docs_search",
      "mcp__livekit-docs__get_pages",
      "mcp__context7__resolve-library-id",
      "mcp__context7__query-docs",
      "Bash(git --no-pager log --oneline --all --graph)",
      "Bash(printenv:*)",
      "WebSearch",
      "WebFetch(domain:livekit.io)",
      "WebFetch(domain:docs.livekit.io)",
      "Bash(python:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(git push:*)",
      "mcp__plugin_context7_context7__resolve-library-id",
      "mcp__plugin_context7_context7__query-docs",
      "Bash(conda run:*)",
      "Bash(C:/Users/USer/miniconda3/envs/engage/python.exe -c \"import livekit.agents; print\\(livekit.agents.__file__\\)\")",
      "Bash(find:*)",
      "Bash(C:/Users/USer/miniconda3/envs/engage/python.exe:*)",
      "Bash(pip show:*)",
      "Bash(conda list:*)",
      "mcp__livekit-docs__get_changelog",
      "Bash(git checkout:*)",
      "Bash(None to complete the tool silently without requiring a reply from the LLM.\"\nThis prevents the Realtime model from generating two separate messages per\ntool call.\n\nOnly check_intent_and_proceed keeps returning a string \\(score-based branching\\).\n\nAlso: temperature 0.6 â†’ 0.7, prompt rule 17 \\(one message per turn\\).\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "mcp__livekit-docs__get_python_agent_example",
      "Bash(PYTHONIOENCODING=utf-8 python:*)",
      "Bash(your needs, personalizing to you, capturing signals.\"\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(git --no-pager log --oneline -5)",
      "Bash(git --no-pager diff CLAUDE.md)",
      "Bash(git --no-pager diff:*)",
      "WebFetch(domain:platform.openai.com)"
    ]
  },
  "prefersReducedMotion": true
}
